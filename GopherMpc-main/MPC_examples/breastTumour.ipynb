{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XTXA68eR9GV",
        "outputId": "a564cf30-6b52-4539-c446-dd0b8a8aa7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pandas scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yIKpLP_TQo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load the Breast Cancer Wisconsin Dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
        "columns = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
        "data = pd.read_csv(url, header=None, names=columns)\n",
        "\n",
        "# Encode target variable: Malignant (M) = 1, Benign (B) = 0\n",
        "data['Diagnosis'] = data['Diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "# Split features and labels\n",
        "X = data.iloc[:, 2:].values  # Features (30 columns)\n",
        "y = data['Diagnosis'].values  # Labels (binary classification)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "7ZCEDeZjSDAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))\n",
        "\n",
        "# Instantiate the model\n",
        "model = LogisticRegression(input_dim=30)\n"
      ],
      "metadata": {
        "id": "-ys_8IJWSFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # Forward pass\n",
        "        predictions = model(X_batch)\n",
        "        loss = criterion(predictions, y_batch)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGrZZZs8SI_3",
        "outputId": "7bf69f42-0e9f-48d4-de67-272265b38aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.5255\n",
            "Epoch 2/50, Loss: 0.4162\n",
            "Epoch 3/50, Loss: 0.3596\n",
            "Epoch 4/50, Loss: 0.3246\n",
            "Epoch 5/50, Loss: 0.3134\n",
            "Epoch 6/50, Loss: 0.2693\n",
            "Epoch 7/50, Loss: 0.2609\n",
            "Epoch 8/50, Loss: 0.2422\n",
            "Epoch 9/50, Loss: 0.2253\n",
            "Epoch 10/50, Loss: 0.2200\n",
            "Epoch 11/50, Loss: 0.2055\n",
            "Epoch 12/50, Loss: 0.1987\n",
            "Epoch 13/50, Loss: 0.1999\n",
            "Epoch 14/50, Loss: 0.1860\n",
            "Epoch 15/50, Loss: 0.1842\n",
            "Epoch 16/50, Loss: 0.1897\n",
            "Epoch 17/50, Loss: 0.1761\n",
            "Epoch 18/50, Loss: 0.1686\n",
            "Epoch 19/50, Loss: 0.1615\n",
            "Epoch 20/50, Loss: 0.1690\n",
            "Epoch 21/50, Loss: 0.1570\n",
            "Epoch 22/50, Loss: 0.1612\n",
            "Epoch 23/50, Loss: 0.1551\n",
            "Epoch 24/50, Loss: 0.1530\n",
            "Epoch 25/50, Loss: 0.1565\n",
            "Epoch 26/50, Loss: 0.1459\n",
            "Epoch 27/50, Loss: 0.1597\n",
            "Epoch 28/50, Loss: 0.1424\n",
            "Epoch 29/50, Loss: 0.1360\n",
            "Epoch 30/50, Loss: 0.1391\n",
            "Epoch 31/50, Loss: 0.1354\n",
            "Epoch 32/50, Loss: 0.1384\n",
            "Epoch 33/50, Loss: 0.1304\n",
            "Epoch 34/50, Loss: 0.1293\n",
            "Epoch 35/50, Loss: 0.1368\n",
            "Epoch 36/50, Loss: 0.1310\n",
            "Epoch 37/50, Loss: 0.1354\n",
            "Epoch 38/50, Loss: 0.1336\n",
            "Epoch 39/50, Loss: 0.1241\n",
            "Epoch 40/50, Loss: 0.1238\n",
            "Epoch 41/50, Loss: 0.1208\n",
            "Epoch 42/50, Loss: 0.1226\n",
            "Epoch 43/50, Loss: 0.1207\n",
            "Epoch 44/50, Loss: 0.1190\n",
            "Epoch 45/50, Loss: 0.1169\n",
            "Epoch 46/50, Loss: 0.1237\n",
            "Epoch 47/50, Loss: 0.1145\n",
            "Epoch 48/50, Loss: 0.1389\n",
            "Epoch 49/50, Loss: 0.1214\n",
            "Epoch 50/50, Loss: 0.1149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        predictions = model(X_batch)\n",
        "        predictions = (predictions >= 0.5).float()  # Convert probabilities to binary labels\n",
        "        correct += (predictions == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46JbcwzBSK4n",
        "outputId": "4aefa6c1-96e8-40cd-a714-577aba59a0f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 98.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First cell - Install just the required packages\n",
        "!pip install torch>=1.7.0\n",
        "!pip install torchvision>=0.9.1\n",
        "!pip install omegaconf>=2.0.6\n",
        "!pip install onnx>=1.7.0\n",
        "!pip install pandas>=1.2.2\n",
        "!pip install pyyaml>=5.3.1\n",
        "!pip install tensorboard\n",
        "!pip install future\n",
        "!pip install scipy>=1.6.0\n",
        "!pip install scikit-learn  # Modern replacement for sklearn\n",
        "\n",
        "!pip install crypten --no-deps  # Install crypten without extra dependencies\n"
      ],
      "metadata": {
        "id": "xXkuYek7VbVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd34db71-adf3-4a43-ef7b-df2a1f161b68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting crypten\n",
            "  Downloading crypten-0.4.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Downloading crypten-0.4.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.9/259.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: crypten\n",
            "Successfully installed crypten-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import crypten\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize CrypTen\n",
        "crypten.init()\n",
        "\n",
        "# Define the MPC workflow using the @crypten.mpc.run_multiprocess decorator\n",
        "@crypten.mpc.run_multiprocess(world_size=3)\n",
        "def train_mpc_model():\n",
        "    # Load the Breast Cancer Wisconsin Dataset\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
        "    columns = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
        "    data = pd.read_csv(url, header=None, names=columns)\n",
        "\n",
        "    # Encode target variable: Malignant (M) = 1, Benign (B) = 0\n",
        "    data['Diagnosis'] = data['Diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "    # Split features and labels\n",
        "    X = data.iloc[:, 2:].values  # Features (30 columns)\n",
        "    y = data['Diagnosis'].values  # Labels (binary classification)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Partition training data among 3 parties\n",
        "    X_party1, X_temp, y_party1, y_temp = train_test_split(X_train, y_train, test_size=0.67, random_state=42)\n",
        "    X_party2, X_party3, y_party2, y_party3 = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_party1_tensor = torch.tensor(X_party1, dtype=torch.float32)\n",
        "    y_party1_tensor = torch.tensor(y_party1, dtype=torch.float32).unsqueeze(1)\n",
        "    X_party2_tensor = torch.tensor(X_party2, dtype=torch.float32)\n",
        "    y_party2_tensor = torch.tensor(y_party2, dtype=torch.float32).unsqueeze(1)\n",
        "    X_party3_tensor = torch.tensor(X_party3, dtype=torch.float32)\n",
        "    y_party3_tensor = torch.tensor(y_party3, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Encrypt data using CrypTen for 3 parties\n",
        "    encrypted_X_party1 = crypten.cryptensor(X_party1_tensor, src=0)\n",
        "    encrypted_y_party1 = crypten.cryptensor(y_party1_tensor, src=0)\n",
        "\n",
        "    encrypted_X_party2 = crypten.cryptensor(X_party2_tensor, src=1)\n",
        "    encrypted_y_party2 = crypten.cryptensor(y_party2_tensor, src=1)\n",
        "\n",
        "    encrypted_X_party3 = crypten.cryptensor(X_party3_tensor, src=2)\n",
        "    encrypted_y_party3 = crypten.cryptensor(y_party3_tensor, src=2)\n",
        "\n",
        "    # Combine encrypted data\n",
        "    encrypted_X_train = crypten.cat([encrypted_X_party1, encrypted_X_party2, encrypted_X_party3], dim=0)\n",
        "    encrypted_y_train = crypten.cat([encrypted_y_party1, encrypted_y_party2, encrypted_y_party3], dim=0)\n",
        "\n",
        "    # Define logistic regression model using PyTorch\n",
        "    class LogisticRegression(torch.nn.Module):\n",
        "        def __init__(self, input_dim):\n",
        "            super(LogisticRegression, self).__init__()\n",
        "            self.linear = torch.nn.Linear(input_dim, 1)\n",
        "            self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.sigmoid(self.linear(x))\n",
        "\n",
        "    # Create and encrypt the model\n",
        "    model = LogisticRegression(input_dim=30)\n",
        "    encrypted_model = crypten.nn.from_pytorch(model, torch.empty(1, 30))\n",
        "    encrypted_model.encrypt()\n",
        "\n",
        "    # Define loss function\n",
        "    loss_fn = crypten.nn.BCELoss()\n",
        "\n",
        "    # Set precision and learning parameters\n",
        "    crypten.encoder.fixed_point_precision = 32\n",
        "    learning_rate = 0.1\n",
        "    num_epochs = 50\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        predictions = encrypted_model(encrypted_X_train)\n",
        "        loss = loss_fn(predictions, encrypted_y_train)\n",
        "\n",
        "        # Backward pass\n",
        "        encrypted_model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters with learning rate decay\n",
        "        lr = learning_rate / (1 + epoch * 0.1)  # Decay learning rate\n",
        "        encrypted_model.update_parameters(lr)\n",
        "\n",
        "        # Print decrypted loss for monitoring\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss.get_plain_text().item():.4f}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    encrypted_model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = encrypted_model(crypten.cryptensor(X_test_tensor, src=0))\n",
        "        decrypted_predictions = predictions.get_plain_text()\n",
        "        binary_predictions = (decrypted_predictions >= 0.5).float()\n",
        "        accuracy = (binary_predictions == y_test_tensor).float().mean().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Run the MPC model training\n",
        "train_mpc_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szr9eRuPchC2",
        "outputId": "44a5f994-916c-463b-cf92-f8a237eedebb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/crypten/__init__.py:64: RuntimeWarning: CrypTen is already initialized.\n",
            "  warnings.warn(\"CrypTen is already initialized.\", RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7247\n",
            "Epoch 1, Loss: 0.7247Epoch 1, Loss: 0.7247\n",
            "\n",
            "Epoch 2, Loss: 0.5724Epoch 2, Loss: 0.5724Epoch 2, Loss: 0.5724\n",
            "\n",
            "\n",
            "Epoch 3, Loss: 0.4949Epoch 3, Loss: 0.4949Epoch 3, Loss: 0.4949\n",
            "\n",
            "\n",
            "Epoch 4, Loss: 0.4488Epoch 4, Loss: 0.4488Epoch 4, Loss: 0.4488\n",
            "\n",
            "\n",
            "Epoch 5, Loss: 0.4177Epoch 5, Loss: 0.4177\n",
            "Epoch 5, Loss: 0.4177\n",
            "\n",
            "Epoch 6, Loss: 0.3939\n",
            "Epoch 6, Loss: 0.3939\n",
            "Epoch 6, Loss: 0.3939\n",
            "Epoch 7, Loss: 0.3760Epoch 7, Loss: 0.3760\n",
            "\n",
            "Epoch 7, Loss: 0.3760\n",
            "Epoch 8, Loss: 0.3617Epoch 8, Loss: 0.3617\n",
            "Epoch 8, Loss: 0.3617\n",
            "\n",
            "Epoch 9, Loss: 0.3496Epoch 9, Loss: 0.3496Epoch 9, Loss: 0.3496\n",
            "\n",
            "\n",
            "Epoch 10, Loss: 0.3395\n",
            "Epoch 10, Loss: 0.3395Epoch 10, Loss: 0.3395\n",
            "\n",
            "Epoch 11, Loss: 0.3308Epoch 11, Loss: 0.3308Epoch 11, Loss: 0.3308\n",
            "\n",
            "\n",
            "Epoch 12, Loss: 0.3238Epoch 12, Loss: 0.3238Epoch 12, Loss: 0.3238\n",
            "\n",
            "\n",
            "Epoch 13, Loss: 0.3168Epoch 13, Loss: 0.3168Epoch 13, Loss: 0.3168\n",
            "\n",
            "\n",
            "Epoch 14, Loss: 0.3116Epoch 14, Loss: 0.3116Epoch 14, Loss: 0.3116\n",
            "\n",
            "\n",
            "Epoch 15, Loss: 0.3064Epoch 15, Loss: 0.3064Epoch 15, Loss: 0.3064\n",
            "\n",
            "\n",
            "Epoch 16, Loss: 0.3017Epoch 16, Loss: 0.3017\n",
            "Epoch 16, Loss: 0.3017\n",
            "\n",
            "Epoch 17, Loss: 0.2973Epoch 17, Loss: 0.2973Epoch 17, Loss: 0.2973\n",
            "\n",
            "\n",
            "Epoch 18, Loss: 0.2927Epoch 18, Loss: 0.2927\n",
            "Epoch 18, Loss: 0.2927\n",
            "\n",
            "Epoch 19, Loss: 0.2897Epoch 19, Loss: 0.2897Epoch 19, Loss: 0.2897\n",
            "\n",
            "\n",
            "Epoch 20, Loss: 0.2866Epoch 20, Loss: 0.2866Epoch 20, Loss: 0.2866\n",
            "\n",
            "\n",
            "Epoch 21, Loss: 0.2834Epoch 21, Loss: 0.2834\n",
            "\n",
            "Epoch 21, Loss: 0.2834\n",
            "Epoch 22, Loss: 0.2804Epoch 22, Loss: 0.2804\n",
            "Epoch 22, Loss: 0.2804\n",
            "\n",
            "Epoch 23, Loss: 0.2780Epoch 23, Loss: 0.2780Epoch 23, Loss: 0.2780\n",
            "\n",
            "\n",
            "Epoch 24, Loss: 0.2755Epoch 24, Loss: 0.2755\n",
            "Epoch 24, Loss: 0.2755\n",
            "\n",
            "Epoch 25, Loss: 0.2735Epoch 25, Loss: 0.2735\n",
            "Epoch 25, Loss: 0.2735\n",
            "\n",
            "Epoch 26, Loss: 0.2710Epoch 26, Loss: 0.2710Epoch 26, Loss: 0.2710\n",
            "\n",
            "\n",
            "Epoch 27, Loss: 0.2688Epoch 27, Loss: 0.2688Epoch 27, Loss: 0.2688\n",
            "\n",
            "\n",
            "Epoch 28, Loss: 0.2670Epoch 28, Loss: 0.2670Epoch 28, Loss: 0.2670\n",
            "\n",
            "\n",
            "Epoch 29, Loss: 0.2649Epoch 29, Loss: 0.2649Epoch 29, Loss: 0.2649\n",
            "\n",
            "\n",
            "Epoch 30, Loss: 0.2636Epoch 30, Loss: 0.2636\n",
            "\n",
            "Epoch 30, Loss: 0.2636\n",
            "Epoch 31, Loss: 0.2618Epoch 31, Loss: 0.2618Epoch 31, Loss: 0.2618\n",
            "\n",
            "\n",
            "Epoch 32, Loss: 0.2606Epoch 32, Loss: 0.2606Epoch 32, Loss: 0.2606\n",
            "\n",
            "\n",
            "Epoch 33, Loss: 0.2592Epoch 33, Loss: 0.2592Epoch 33, Loss: 0.2592\n",
            "\n",
            "\n",
            "Epoch 34, Loss: 0.2574Epoch 34, Loss: 0.2574Epoch 34, Loss: 0.2574\n",
            "\n",
            "\n",
            "Epoch 35, Loss: 0.2559Epoch 35, Loss: 0.2559Epoch 35, Loss: 0.2559\n",
            "\n",
            "\n",
            "Epoch 36, Loss: 0.2551Epoch 36, Loss: 0.2551Epoch 36, Loss: 0.2551\n",
            "\n",
            "\n",
            "Epoch 37, Loss: 0.2533\n",
            "Epoch 37, Loss: 0.2533\n",
            "Epoch 37, Loss: 0.2533\n",
            "Epoch 38, Loss: 0.2522Epoch 38, Loss: 0.2522\n",
            "\n",
            "Epoch 38, Loss: 0.2522\n",
            "Epoch 39, Loss: 0.2507\n",
            "Epoch 39, Loss: 0.2507Epoch 39, Loss: 0.2507\n",
            "\n",
            "Epoch 40, Loss: 0.2498Epoch 40, Loss: 0.2498Epoch 40, Loss: 0.2498\n",
            "\n",
            "\n",
            "Epoch 41, Loss: 0.2489\n",
            "Epoch 41, Loss: 0.2489Epoch 41, Loss: 0.2489\n",
            "\n",
            "Epoch 42, Loss: 0.2480\n",
            "Epoch 42, Loss: 0.2480Epoch 42, Loss: 0.2480\n",
            "\n",
            "Epoch 43, Loss: 0.2468Epoch 43, Loss: 0.2468Epoch 43, Loss: 0.2468\n",
            "\n",
            "\n",
            "Epoch 44, Loss: 0.2460Epoch 44, Loss: 0.2460Epoch 44, Loss: 0.2460\n",
            "\n",
            "\n",
            "Epoch 45, Loss: 0.2443\n",
            "Epoch 45, Loss: 0.2443\n",
            "Epoch 45, Loss: 0.2443\n",
            "Epoch 46, Loss: 0.2439Epoch 46, Loss: 0.2439Epoch 46, Loss: 0.2439\n",
            "\n",
            "\n",
            "Epoch 47, Loss: 0.2431\n",
            "Epoch 47, Loss: 0.2431Epoch 47, Loss: 0.2431\n",
            "\n",
            "Epoch 48, Loss: 0.2423Epoch 48, Loss: 0.2423Epoch 48, Loss: 0.2423\n",
            "\n",
            "\n",
            "Epoch 49, Loss: 0.2412Epoch 49, Loss: 0.2412\n",
            "Epoch 49, Loss: 0.2412\n",
            "\n",
            "Epoch 50, Loss: 0.2404\n",
            "Epoch 50, Loss: 0.2404Epoch 50, Loss: 0.2404\n",
            "\n",
            "Test Accuracy: 97.37%Test Accuracy: 97.37%Test Accuracy: 97.37%\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install crypten --no-deps\n",
        "!pip install pandas scikit-learn torch torchvision\n",
        "\n",
        "# Import libraries\n",
        "import crypten\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize CrypTen\n",
        "crypten.init()\n",
        "\n",
        "# Define the MPC workflow using the @crypten.mpc.run_multiprocess decorator\n",
        "@crypten.mpc.run_multiprocess(world_size=3)\n",
        "def train_mpc_model():\n",
        "    # Load the Breast Cancer Wisconsin Dataset\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
        "    columns = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
        "    data = pd.read_csv(url, header=None, names=columns)\n",
        "\n",
        "    # Encode target variable: Malignant (M) = 1, Benign (B) = 0\n",
        "    data['Diagnosis'] = data['Diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "    # Split features and labels\n",
        "    X = data.iloc[:, 2:].values  # Features (30 columns)\n",
        "    y = data['Diagnosis'].values  # Labels (binary classification)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Vertical splitting: divide features among 3 parties\n",
        "    # Split X_train and X_test into 3 parts (columns split)\n",
        "    X_party1 = X_train[:, :10]  # First 10 features\n",
        "    X_party2 = X_train[:, 10:20]  # Next 10 features\n",
        "    X_party3 = X_train[:, 20:]  # Last 10 features\n",
        "\n",
        "    # Split test data similarly\n",
        "    X_test_party1 = X_test[:, :10]\n",
        "    X_test_party2 = X_test[:, 10:20]\n",
        "    X_test_party3 = X_test[:, 20:]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_party1_tensor = torch.tensor(X_party1, dtype=torch.float32)\n",
        "    X_party2_tensor = torch.tensor(X_party2, dtype=torch.float32)\n",
        "    X_party3_tensor = torch.tensor(X_party3, dtype=torch.float32)\n",
        "\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_party1_tensor = torch.tensor(X_test_party1, dtype=torch.float32)\n",
        "    X_test_party2_tensor = torch.tensor(X_test_party2, dtype=torch.float32)\n",
        "    X_test_party3_tensor = torch.tensor(X_test_party3, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Encrypt data using CrypTen for 3 parties\n",
        "    encrypted_X_party1 = crypten.cryptensor(X_party1_tensor, src=0)\n",
        "    encrypted_X_party2 = crypten.cryptensor(X_party2_tensor, src=1)\n",
        "    encrypted_X_party3 = crypten.cryptensor(X_party3_tensor, src=2)\n",
        "    encrypted_y_train = crypten.cryptensor(y_train_tensor, src=0)  # Labels shared only once\n",
        "\n",
        "    # Combine encrypted data (horizontally stack features back together)\n",
        "    encrypted_X_train = crypten.cat([encrypted_X_party1, encrypted_X_party2, encrypted_X_party3], dim=1)\n",
        "\n",
        "    # Define logistic regression model using PyTorch\n",
        "    class LogisticRegression(torch.nn.Module):\n",
        "        def __init__(self, input_dim):\n",
        "            super(LogisticRegression, self).__init__()\n",
        "            self.linear = torch.nn.Linear(input_dim, 1)\n",
        "            self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.sigmoid(self.linear(x))\n",
        "\n",
        "    # Create and encrypt the model\n",
        "    model = LogisticRegression(input_dim=30)\n",
        "    encrypted_model = crypten.nn.from_pytorch(model, torch.empty(1, 30))\n",
        "    encrypted_model.encrypt()\n",
        "\n",
        "    # Define loss function\n",
        "    loss_fn = crypten.nn.BCELoss()\n",
        "\n",
        "    # Set precision and learning parameters\n",
        "    crypten.encoder.fixed_point_precision = 32\n",
        "    learning_rate = 0.1\n",
        "    num_epochs = 50\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        predictions = encrypted_model(encrypted_X_train)\n",
        "        loss = loss_fn(predictions, encrypted_y_train)\n",
        "\n",
        "        # Backward pass\n",
        "        encrypted_model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters with learning rate decay\n",
        "        lr = learning_rate / (1 + epoch * 0.1)  # Decay learning rate\n",
        "        encrypted_model.update_parameters(lr)\n",
        "\n",
        "        # Print decrypted loss for monitoring\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss.get_plain_text().item():.4f}\")\n",
        "\n",
        "    # Combine encrypted test data\n",
        "    encrypted_X_test = crypten.cat([crypten.cryptensor(X_test_party1_tensor, src=0),\n",
        "                                     crypten.cryptensor(X_test_party2_tensor, src=1),\n",
        "                                     crypten.cryptensor(X_test_party3_tensor, src=2)], dim=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    encrypted_model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = encrypted_model(encrypted_X_test)\n",
        "        decrypted_predictions = predictions.get_plain_text()\n",
        "        binary_predictions = (decrypted_predictions >= 0.5).float()\n",
        "        accuracy = (binary_predictions == y_test_tensor).float().mean().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Run the MPC model training\n",
        "train_mpc_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I46RhbGLr6KD",
        "outputId": "69473842-cc0a-4c8e-e882-3668e1e5ba98"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: crypten in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Epoch 1, Loss: 0.7252Epoch 1, Loss: 0.7252Epoch 1, Loss: 0.7252\n",
            "\n",
            "\n",
            "Epoch 2, Loss: 0.5720\n",
            "Epoch 2, Loss: 0.5720\n",
            "Epoch 2, Loss: 0.5720\n",
            "Epoch 3, Loss: 0.4953Epoch 3, Loss: 0.4953Epoch 3, Loss: 0.4953\n",
            "\n",
            "\n",
            "Epoch 4, Loss: 0.4490Epoch 4, Loss: 0.4490Epoch 4, Loss: 0.4490\n",
            "\n",
            "\n",
            "Epoch 5, Loss: 0.4173Epoch 5, Loss: 0.4173Epoch 5, Loss: 0.4173\n",
            "\n",
            "\n",
            "Epoch 6, Loss: 0.3936Epoch 6, Loss: 0.3936Epoch 6, Loss: 0.3936\n",
            "\n",
            "\n",
            "Epoch 7, Loss: 0.3757Epoch 7, Loss: 0.3757\n",
            "Epoch 7, Loss: 0.3757\n",
            "\n",
            "Epoch 8, Loss: 0.3612Epoch 8, Loss: 0.3612\n",
            "Epoch 8, Loss: 0.3612\n",
            "\n",
            "Epoch 9, Loss: 0.3493Epoch 9, Loss: 0.3493Epoch 9, Loss: 0.3493\n",
            "\n",
            "\n",
            "Epoch 10, Loss: 0.3395Epoch 10, Loss: 0.3395Epoch 10, Loss: 0.3395\n",
            "\n",
            "\n",
            "Epoch 11, Loss: 0.3313Epoch 11, Loss: 0.3313Epoch 11, Loss: 0.3313\n",
            "\n",
            "\n",
            "Epoch 12, Loss: 0.3237Epoch 12, Loss: 0.3237Epoch 12, Loss: 0.3237\n",
            "\n",
            "\n",
            "Epoch 13, Loss: 0.3169Epoch 13, Loss: 0.3169Epoch 13, Loss: 0.3169\n",
            "\n",
            "\n",
            "Epoch 14, Loss: 0.3114Epoch 14, Loss: 0.3114Epoch 14, Loss: 0.3114\n",
            "\n",
            "\n",
            "Epoch 15, Loss: 0.3060Epoch 15, Loss: 0.3060Epoch 15, Loss: 0.3060\n",
            "\n",
            "\n",
            "Epoch 16, Loss: 0.3014Epoch 16, Loss: 0.3014Epoch 16, Loss: 0.3014\n",
            "\n",
            "\n",
            "Epoch 17, Loss: 0.2974\n",
            "Epoch 17, Loss: 0.2974Epoch 17, Loss: 0.2974\n",
            "\n",
            "Epoch 18, Loss: 0.2930Epoch 18, Loss: 0.2930Epoch 18, Loss: 0.2930\n",
            "\n",
            "\n",
            "Epoch 19, Loss: 0.2897Epoch 19, Loss: 0.2897Epoch 19, Loss: 0.2897\n",
            "\n",
            "\n",
            "Epoch 20, Loss: 0.2865Epoch 20, Loss: 0.2865Epoch 20, Loss: 0.2865\n",
            "\n",
            "\n",
            "Epoch 21, Loss: 0.2833Epoch 21, Loss: 0.2833Epoch 21, Loss: 0.2833\n",
            "\n",
            "\n",
            "Epoch 22, Loss: 0.2804Epoch 22, Loss: 0.2804Epoch 22, Loss: 0.2804\n",
            "\n",
            "\n",
            "Epoch 23, Loss: 0.2776\n",
            "Epoch 23, Loss: 0.2776Epoch 23, Loss: 0.2776\n",
            "\n",
            "Epoch 24, Loss: 0.2753Epoch 24, Loss: 0.2753Epoch 24, Loss: 0.2753\n",
            "\n",
            "\n",
            "Epoch 25, Loss: 0.2730Epoch 25, Loss: 0.2730\n",
            "Epoch 25, Loss: 0.2730\n",
            "\n",
            "Epoch 26, Loss: 0.2708\n",
            "Epoch 26, Loss: 0.2708Epoch 26, Loss: 0.2708\n",
            "\n",
            "Epoch 27, Loss: 0.2690Epoch 27, Loss: 0.2690\n",
            "\n",
            "Epoch 27, Loss: 0.2690\n",
            "Epoch 28, Loss: 0.2667\n",
            "Epoch 28, Loss: 0.2667Epoch 28, Loss: 0.2667\n",
            "\n",
            "Epoch 29, Loss: 0.2652Epoch 29, Loss: 0.2652Epoch 29, Loss: 0.2652\n",
            "\n",
            "\n",
            "Epoch 30, Loss: 0.2631Epoch 30, Loss: 0.2631Epoch 30, Loss: 0.2631\n",
            "\n",
            "\n",
            "Epoch 31, Loss: 0.2617Epoch 31, Loss: 0.2617Epoch 31, Loss: 0.2617\n",
            "\n",
            "\n",
            "Epoch 32, Loss: 0.2602Epoch 32, Loss: 0.2602Epoch 32, Loss: 0.2602\n",
            "\n",
            "\n",
            "Epoch 33, Loss: 0.2588Epoch 33, Loss: 0.2588Epoch 33, Loss: 0.2588\n",
            "\n",
            "\n",
            "Epoch 34, Loss: 0.2576\n",
            "Epoch 34, Loss: 0.2576Epoch 34, Loss: 0.2576\n",
            "\n",
            "Epoch 35, Loss: 0.2557Epoch 35, Loss: 0.2557Epoch 35, Loss: 0.2557\n",
            "\n",
            "\n",
            "Epoch 36, Loss: 0.2549Epoch 36, Loss: 0.2549\n",
            "\n",
            "Epoch 36, Loss: 0.2549\n",
            "Epoch 37, Loss: 0.2533\n",
            "Epoch 37, Loss: 0.2533Epoch 37, Loss: 0.2533\n",
            "\n",
            "Epoch 38, Loss: 0.2525Epoch 38, Loss: 0.2525\n",
            "Epoch 38, Loss: 0.2525\n",
            "\n",
            "Epoch 39, Loss: 0.2509Epoch 39, Loss: 0.2509\n",
            "Epoch 39, Loss: 0.2509\n",
            "\n",
            "Epoch 40, Loss: 0.2497Epoch 40, Loss: 0.2497Epoch 40, Loss: 0.2497\n",
            "\n",
            "\n",
            "Epoch 41, Loss: 0.2487Epoch 41, Loss: 0.2487Epoch 41, Loss: 0.2487\n",
            "\n",
            "\n",
            "Epoch 42, Loss: 0.2477Epoch 42, Loss: 0.2477Epoch 42, Loss: 0.2477\n",
            "\n",
            "\n",
            "Epoch 43, Loss: 0.2469Epoch 43, Loss: 0.2469Epoch 43, Loss: 0.2469\n",
            "\n",
            "\n",
            "Epoch 44, Loss: 0.2457Epoch 44, Loss: 0.2457Epoch 44, Loss: 0.2457\n",
            "\n",
            "\n",
            "Epoch 45, Loss: 0.2449Epoch 45, Loss: 0.2449Epoch 45, Loss: 0.2449\n",
            "\n",
            "\n",
            "Epoch 46, Loss: 0.2443\n",
            "Epoch 46, Loss: 0.2443Epoch 46, Loss: 0.2443\n",
            "\n",
            "Epoch 47, Loss: 0.2430Epoch 47, Loss: 0.2430\n",
            "\n",
            "Epoch 47, Loss: 0.2430\n",
            "Epoch 48, Loss: 0.2424Epoch 48, Loss: 0.2424Epoch 48, Loss: 0.2424\n",
            "\n",
            "\n",
            "Epoch 49, Loss: 0.2413Epoch 49, Loss: 0.2413Epoch 49, Loss: 0.2413\n",
            "\n",
            "\n",
            "Epoch 50, Loss: 0.2408Epoch 50, Loss: 0.2408\n",
            "Epoch 50, Loss: 0.2408\n",
            "\n",
            "Test Accuracy: 97.37%\n",
            "Test Accuracy: 97.37%\n",
            "Test Accuracy: 97.37%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install crypten --no-deps\n",
        "!pip install pandas scikit-learn torch torchvision\n",
        "\n",
        "# Import libraries\n",
        "import crypten\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize CrypTen\n",
        "crypten.init()\n",
        "\n",
        "# Define the MPC workflow using the @crypten.mpc.run_multiprocess decorator\n",
        "@crypten.mpc.run_multiprocess(world_size=3)\n",
        "def train_mpc_model():\n",
        "    # Load the Breast Cancer Wisconsin Dataset\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
        "    columns = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
        "    data = pd.read_csv(url, header=None, names=columns)\n",
        "\n",
        "    # Encode target variable: Malignant (M) = 1, Benign (B) = 0\n",
        "    data['Diagnosis'] = data['Diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "    # Split features and labels\n",
        "    X = data.iloc[:, 2:].values  # Features (30 columns)\n",
        "    y = data['Diagnosis'].values  # Labels (binary classification)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Vertical splitting: divide features among 3 parties\n",
        "    # Split X_train and X_test into 3 parts (columns split)\n",
        "    X_party1 = X_train[:, :10]  # First 10 features\n",
        "    X_party2 = X_train[:, 10:20]  # Next 10 features\n",
        "    X_party3 = X_train[:, 20:]  # Last 10 features\n",
        "\n",
        "    # Split test data similarly\n",
        "    X_test_party1 = X_test[:, :10]\n",
        "    X_test_party2 = X_test[:, 10:20]\n",
        "    X_test_party3 = X_test[:, 20:]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_party1_tensor = torch.tensor(X_party1, dtype=torch.float32)\n",
        "    X_party2_tensor = torch.tensor(X_party2, dtype=torch.float32)\n",
        "    X_party3_tensor = torch.tensor(X_party3, dtype=torch.float32)\n",
        "\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_party1_tensor = torch.tensor(X_test_party1, dtype=torch.float32)\n",
        "    X_test_party2_tensor = torch.tensor(X_test_party2, dtype=torch.float32)\n",
        "    X_test_party3_tensor = torch.tensor(X_test_party3, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Encrypt data using CrypTen for 3 parties\n",
        "    encrypted_X_party1 = crypten.cryptensor(X_party1_tensor, src=0)\n",
        "    encrypted_X_party2 = crypten.cryptensor(X_party2_tensor, src=1)\n",
        "    encrypted_X_party3 = crypten.cryptensor(X_party3_tensor, src=2)\n",
        "    encrypted_y_train = crypten.cryptensor(y_train_tensor, src=0)  # Labels shared only once\n",
        "\n",
        "    # Combine encrypted data (horizontally stack features back together)\n",
        "    encrypted_X_train = crypten.cat([encrypted_X_party1, encrypted_X_party2, encrypted_X_party3], dim=1)\n",
        "\n",
        "    # Define logistic regression model using PyTorch\n",
        "    class LogisticRegression(torch.nn.Module):\n",
        "        def __init__(self, input_dim):\n",
        "            super(LogisticRegression, self).__init__()\n",
        "            self.linear = torch.nn.Linear(input_dim, 1)\n",
        "            self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.sigmoid(self.linear(x))\n",
        "\n",
        "    # Create and encrypt the model\n",
        "    model = LogisticRegression(input_dim=30)\n",
        "    encrypted_model = crypten.nn.from_pytorch(model, torch.empty(1, 30))\n",
        "    encrypted_model.encrypt()\n",
        "\n",
        "    # Define loss function\n",
        "    loss_fn = crypten.nn.BCELoss()\n",
        "\n",
        "    # Set precision and learning parameters\n",
        "    crypten.encoder.fixed_point_precision = 64\n",
        "    learning_rate = 0.1\n",
        "    num_epochs = 50\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        predictions = encrypted_model(encrypted_X_train)\n",
        "        loss = loss_fn(predictions, encrypted_y_train)\n",
        "\n",
        "        # Backward pass\n",
        "        encrypted_model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters with learning rate decay\n",
        "        lr = learning_rate / (1 + epoch * 0.1)  # Decay learning rate\n",
        "        encrypted_model.update_parameters(lr)\n",
        "\n",
        "        # Print decrypted loss for monitoring\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss.get_plain_text().item():.4f}\")\n",
        "\n",
        "    # Combine encrypted test data\n",
        "    encrypted_X_test = crypten.cat([crypten.cryptensor(X_test_party1_tensor, src=0),\n",
        "                                     crypten.cryptensor(X_test_party2_tensor, src=1),\n",
        "                                     crypten.cryptensor(X_test_party3_tensor, src=2)], dim=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    encrypted_model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = encrypted_model(encrypted_X_test)\n",
        "        decrypted_predictions = predictions.get_plain_text()\n",
        "        binary_predictions = (decrypted_predictions >= 0.5).float()\n",
        "        accuracy = (binary_predictions == y_test_tensor).float().mean().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Run the MPC model training\n",
        "train_mpc_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbx3_iF3tE6E",
        "outputId": "6dca4e26-dfb8-48e3-8980-f7c6e9413696"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: crypten in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Epoch 1, Loss: 0.7248Epoch 1, Loss: 0.7248\n",
            "Epoch 1, Loss: 0.7248\n",
            "\n",
            "Epoch 2, Loss: 0.5725Epoch 2, Loss: 0.5725Epoch 2, Loss: 0.5725\n",
            "\n",
            "\n",
            "Epoch 3, Loss: 0.4955Epoch 3, Loss: 0.4955Epoch 3, Loss: 0.4955\n",
            "\n",
            "\n",
            "Epoch 4, Loss: 0.4488Epoch 4, Loss: 0.4488Epoch 4, Loss: 0.4488\n",
            "\n",
            "\n",
            "Epoch 5, Loss: 0.4172\n",
            "Epoch 5, Loss: 0.4172Epoch 5, Loss: 0.4172\n",
            "\n",
            "Epoch 6, Loss: 0.3937Epoch 6, Loss: 0.3937Epoch 6, Loss: 0.3937\n",
            "\n",
            "\n",
            "Epoch 7, Loss: 0.3762\n",
            "Epoch 7, Loss: 0.3762Epoch 7, Loss: 0.3762\n",
            "\n",
            "Epoch 8, Loss: 0.3615Epoch 8, Loss: 0.3615Epoch 8, Loss: 0.3615\n",
            "\n",
            "\n",
            "Epoch 9, Loss: 0.3500Epoch 9, Loss: 0.3500Epoch 9, Loss: 0.3500\n",
            "\n",
            "\n",
            "Epoch 10, Loss: 0.3398Epoch 10, Loss: 0.3398Epoch 10, Loss: 0.3398\n",
            "\n",
            "\n",
            "Epoch 11, Loss: 0.3313\n",
            "Epoch 11, Loss: 0.3313Epoch 11, Loss: 0.3313\n",
            "\n",
            "Epoch 12, Loss: 0.3240Epoch 12, Loss: 0.3240Epoch 12, Loss: 0.3240\n",
            "\n",
            "\n",
            "Epoch 13, Loss: 0.3173Epoch 13, Loss: 0.3173Epoch 13, Loss: 0.3173\n",
            "\n",
            "\n",
            "Epoch 14, Loss: 0.3113Epoch 14, Loss: 0.3113Epoch 14, Loss: 0.3113\n",
            "\n",
            "\n",
            "Epoch 15, Loss: 0.3063Epoch 15, Loss: 0.3063Epoch 15, Loss: 0.3063\n",
            "\n",
            "\n",
            "Epoch 16, Loss: 0.3014Epoch 16, Loss: 0.3014\n",
            "Epoch 16, Loss: 0.3014\n",
            "\n",
            "Epoch 17, Loss: 0.2973Epoch 17, Loss: 0.2973\n",
            "Epoch 17, Loss: 0.2973\n",
            "\n",
            "Epoch 18, Loss: 0.2939Epoch 18, Loss: 0.2939\n",
            "Epoch 18, Loss: 0.2939\n",
            "\n",
            "Epoch 19, Loss: 0.2898\n",
            "Epoch 19, Loss: 0.2898Epoch 19, Loss: 0.2898\n",
            "\n",
            "Epoch 20, Loss: 0.2862\n",
            "Epoch 20, Loss: 0.2862Epoch 20, Loss: 0.2862\n",
            "\n",
            "Epoch 21, Loss: 0.2837\n",
            "Epoch 21, Loss: 0.2837Epoch 21, Loss: 0.2837\n",
            "\n",
            "Epoch 22, Loss: 0.2802\n",
            "Epoch 22, Loss: 0.2802Epoch 22, Loss: 0.2802\n",
            "\n",
            "Epoch 23, Loss: 0.2779Epoch 23, Loss: 0.2779Epoch 23, Loss: 0.2779\n",
            "\n",
            "\n",
            "Epoch 24, Loss: 0.2763Epoch 24, Loss: 0.2763Epoch 24, Loss: 0.2763\n",
            "\n",
            "\n",
            "Epoch 25, Loss: 0.2730Epoch 25, Loss: 0.2730\n",
            "\n",
            "Epoch 25, Loss: 0.2730\n",
            "Epoch 26, Loss: 0.2713\n",
            "Epoch 26, Loss: 0.2713Epoch 26, Loss: 0.2713\n",
            "\n",
            "Epoch 27, Loss: 0.2690\n",
            "Epoch 27, Loss: 0.2690Epoch 27, Loss: 0.2690\n",
            "\n",
            "Epoch 28, Loss: 0.2675\n",
            "Epoch 28, Loss: 0.2675Epoch 28, Loss: 0.2675\n",
            "\n",
            "Epoch 29, Loss: 0.2652Epoch 29, Loss: 0.2652Epoch 29, Loss: 0.2652\n",
            "\n",
            "\n",
            "Epoch 30, Loss: 0.2636\n",
            "Epoch 30, Loss: 0.2636Epoch 30, Loss: 0.2636\n",
            "\n",
            "Epoch 31, Loss: 0.2617Epoch 31, Loss: 0.2617Epoch 31, Loss: 0.2617\n",
            "\n",
            "\n",
            "Epoch 32, Loss: 0.2604Epoch 32, Loss: 0.2604Epoch 32, Loss: 0.2604\n",
            "\n",
            "\n",
            "Epoch 33, Loss: 0.2588Epoch 33, Loss: 0.2588Epoch 33, Loss: 0.2588\n",
            "\n",
            "\n",
            "Epoch 34, Loss: 0.2574Epoch 34, Loss: 0.2574Epoch 34, Loss: 0.2574\n",
            "\n",
            "\n",
            "Epoch 35, Loss: 0.2558Epoch 35, Loss: 0.2558Epoch 35, Loss: 0.2558\n",
            "\n",
            "\n",
            "Epoch 36, Loss: 0.2548Epoch 36, Loss: 0.2548\n",
            "\n",
            "Epoch 36, Loss: 0.2548\n",
            "Epoch 37, Loss: 0.2532Epoch 37, Loss: 0.2532Epoch 37, Loss: 0.2532\n",
            "\n",
            "\n",
            "Epoch 38, Loss: 0.2522Epoch 38, Loss: 0.2522\n",
            "Epoch 38, Loss: 0.2522\n",
            "\n",
            "Epoch 39, Loss: 0.2512Epoch 39, Loss: 0.2512Epoch 39, Loss: 0.2512\n",
            "\n",
            "\n",
            "Epoch 40, Loss: 0.2498Epoch 40, Loss: 0.2498Epoch 40, Loss: 0.2498\n",
            "\n",
            "\n",
            "Epoch 41, Loss: 0.2491Epoch 41, Loss: 0.2491Epoch 41, Loss: 0.2491\n",
            "\n",
            "\n",
            "Epoch 42, Loss: 0.2474Epoch 42, Loss: 0.2474\n",
            "Epoch 42, Loss: 0.2474\n",
            "\n",
            "Epoch 43, Loss: 0.2465Epoch 43, Loss: 0.2465\n",
            "Epoch 43, Loss: 0.2465\n",
            "\n",
            "Epoch 44, Loss: 0.2459Epoch 44, Loss: 0.2459\n",
            "\n",
            "Epoch 44, Loss: 0.2459\n",
            "Epoch 45, Loss: 0.2448Epoch 45, Loss: 0.2448Epoch 45, Loss: 0.2448\n",
            "\n",
            "\n",
            "Epoch 46, Loss: 0.2440Epoch 46, Loss: 0.2440\n",
            "Epoch 46, Loss: 0.2440\n",
            "\n",
            "Epoch 47, Loss: 0.2435Epoch 47, Loss: 0.2435Epoch 47, Loss: 0.2435\n",
            "\n",
            "\n",
            "Epoch 48, Loss: 0.2427Epoch 48, Loss: 0.2427Epoch 48, Loss: 0.2427\n",
            "\n",
            "\n",
            "Epoch 49, Loss: 0.2411\n",
            "Epoch 49, Loss: 0.2411Epoch 49, Loss: 0.2411\n",
            "\n",
            "Epoch 50, Loss: 0.2402\n",
            "Epoch 50, Loss: 0.2402Epoch 50, Loss: 0.2402\n",
            "\n",
            "Test Accuracy: 96.49%Test Accuracy: 96.49%\n",
            "Test Accuracy: 96.49%\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}